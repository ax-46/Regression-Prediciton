{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the relevant libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing #will be used to standardize the inputs\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the data from the csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD THE CSV FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coma delimiter \n",
    "raw_csv_data = np.loadtxt('scaled_data', delimiter =',')\n",
    "\n",
    "#exclude ID column (useless) and separate inputs and targets\n",
    "unscaled_inputs_all = raw_csv_data[:,:-1]\n",
    "targets_all = raw_csv_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.14734431,  0.06739032, -0.99617478, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-1.14734431,  0.06739032, -0.99617478, ...,  0.        ,\n",
       "         0.        ,  1.        ],\n",
       "       [-1.14734431,  0.06739032, -0.99617478, ...,  1.        ,\n",
       "         0.        ,  1.        ],\n",
       "       ...,\n",
       "       [-0.33081158, -0.40771142,  1.69851383, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.33081158, -0.40771142,  1.69851383, ...,  0.        ,\n",
       "         0.        ,  0.        ],\n",
       "       [-0.33081158, -0.40771142,  1.69851383, ...,  0.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_csv_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Balance the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shuffle the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the indices from axis 0 of our scaled inputs (the target indices are taken indirectly)\n",
    "shuffled_indeces = np.arange(unscaled_inputs_all.shape[0])\n",
    "\n",
    "#shuffle them\n",
    "np.random.shuffle(shuffled_indeces)\n",
    "\n",
    "#rearrange inputs and target following shuffled indices\n",
    "unscaled_inputs_all = unscaled_inputs_all[shuffled_indeces]\n",
    "targets_all = targets_all[shuffled_indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same number of 1 and 0 in the target column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "821"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the number of 1\n",
    "#declare the variable as an int to make sure it is an integer\n",
    "number_of_one_targets = int(np.sum(targets_all))\n",
    "number_of_one_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#count the number of 0\n",
    "#declare the variable as an int to make sure it is an integer\n",
    "number_of_zeros = len(targets_all)-number_of_one_targets\n",
    "number_of_zeros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep as many 0s as we have 1s\n",
    "one_targets_counter = 0\n",
    "indices_to_remove =[] #must be a list or a tuple\n",
    "\n",
    "#count the number of zeroes\n",
    "for i in range(targets_all.shape[0]):\n",
    "    if targets_all[i]==1:\n",
    "        one_targets_counter +=1\n",
    "        if one_targets_counter > number_of_zeros: #when more 0s than 1s are found\n",
    "            indices_to_remove.append(i) #mark the indeces to remove in order to have the same numbers of 0s and 1s\n",
    "\n",
    "#delete inputs and targets corresponding to the marked ibndeces\n",
    "inputs_balanced = np.delete(unscaled_inputs_all, indices_to_remove, axis = 0)\n",
    "targets_balanced = np.delete(targets_all, indices_to_remove, axis = 0)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "307"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "number_of_one_targets = int(np.sum(targets_balanced))\n",
    "number_of_one_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardize the inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALREADY TAKEN CARE OF IN THE LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Shuffle the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always good to shuffle the data to have a random order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take the indices from axis 0 of our scaled inputs (the target indices are taken indirectly)\n",
    "shuffled_indeces = np.arange(inputs_balanced.shape[0])\n",
    "\n",
    "#shuffle them\n",
    "np.random.shuffle(shuffled_indeces)\n",
    "\n",
    "#rearrange inputs and target following shuffled indices\n",
    "shuffled_inputs = inputs_balanced[shuffled_indeces]\n",
    "shuffled_targets = targets_balanced[shuffled_indeces]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the dataset into train, validation, and test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Determine the size of the 3 datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count the total number of samples\n",
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "#80-10-10 split, make sure the count is an integer\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count #avoid rounding errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EXTRACT train, validation, test sets from the big dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#first train_samples_count of the inputs/targets sets are our train set\n",
    "train_inputs = shuffled_inputs[:train_samples_count] #specify the interval from which we extract the set\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "#then the following validation_samples_count of inputs/targets are our validation set\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "#the final test_samples_count are our test set\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:] \n",
    "test_targets = shuffled_targets[train_samples_count+validation_samples_count:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CHECK if the train, validation, and test sets are also BALANCED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples = 614\n",
      "TRAIN SET: number of 1s = 242.0 -- Size = 491 -- % of 1 = 0.49287169042769857\n",
      "VALIDATION SET: number of 1s = 33.0 -- Size = 61 -- % of 1 = 0.5409836065573771\n",
      "TEST SET: number of 1s = 32.0 -- Size = 62 -- % of 1 = 0.5161290322580645\n"
     ]
    }
   ],
   "source": [
    "print('Number of samples =', samples_count)\n",
    "print('TRAIN SET: number of 1s =',np.sum(train_targets), '-- Size =', train_samples_count, '-- % of 1 =', np.sum(train_targets)/train_samples_count)\n",
    "print('VALIDATION SET: number of 1s =',np.sum(validation_targets), '-- Size =', validation_samples_count, '-- % of 1 =', np.sum(validation_targets)/validation_samples_count)\n",
    "print('TEST SET: number of 1s =', np.sum(test_targets), '-- Size =', test_samples_count, '-- % of 1 =',  np.sum(test_targets)/test_samples_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the three datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we use .npz files through np.savez('file name', labels=array_to_save, ..., ...)\n",
    "np.savez('data_train', inputs= train_inputs, targets = train_targets)\n",
    "np.savez('data_validation', inputs= validation_inputs, targets = validation_targets)\n",
    "np.savez('data_test', inputs= test_inputs, targets = test_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD THE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#temporary variable to store train set\n",
    "npz = np.load('data_train.npz')\n",
    "\n",
    "#extract train inputs and targets\n",
    "train_inputs = npz['inputs'].astype(float) #as floats\n",
    "train_targets = npz['targets'].astype(int) #as int\n",
    "\n",
    "\n",
    "#do the same for validation and test set\n",
    "npz = np.load('data_validation.npz')\n",
    "validation_inputs = npz['inputs'].astype(float)\n",
    "validation_targets = npz['targets'].astype(int) \n",
    "\n",
    "npz = np.load('data_test.npz')\n",
    "test_inputs = npz['inputs'].astype(float) \n",
    "test_targets = npz['targets'].astype(int)\n",
    "\n",
    "#train, validation, and test data sets are in Array form"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### THE ACTUAL MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OUTLINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 25 #10 features\n",
    "output_size = 2 #output is 0 or 1\n",
    "hidden_layer_size = 100 #all hidden layers have the same size\n",
    "\n",
    "#build the actual model\n",
    "model = tf.keras.Sequential([\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='relu'), #1st hidden layer\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='tanh'), #2nd hidden layer\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='sigmoid'), #5th hidden layer\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='sigmoid'), #5th hidden layer\n",
    "                            tf.keras.layers.Dense(hidden_layer_size, activation ='relu'), #5th hidden layer\n",
    "                            tf.keras.layers.Dense(output_size, activation ='softmax') #model is a classifier -> softmax\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OPTIMIZER AND LOSS FUNCTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = 'adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BATCH SIZE, NUMBER OF EPOCHS, AND FIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "5/5 - 2s - loss: 0.7065 - accuracy: 0.5642 - val_loss: 0.6668 - val_accuracy: 0.6230\n",
      "Epoch 2/50\n",
      "5/5 - 0s - loss: 0.6887 - accuracy: 0.5132 - val_loss: 0.6889 - val_accuracy: 0.4590\n",
      "Epoch 3/50\n",
      "5/5 - 0s - loss: 0.6778 - accuracy: 0.5784 - val_loss: 0.6480 - val_accuracy: 0.6066\n",
      "Epoch 4/50\n",
      "5/5 - 0s - loss: 0.6572 - accuracy: 0.6436 - val_loss: 0.6446 - val_accuracy: 0.7049\n",
      "Epoch 5/50\n",
      "5/5 - 0s - loss: 0.6554 - accuracy: 0.6314 - val_loss: 0.6265 - val_accuracy: 0.7049\n",
      "Epoch 6/50\n",
      "5/5 - 0s - loss: 0.6329 - accuracy: 0.6680 - val_loss: 0.6110 - val_accuracy: 0.7049\n",
      "Epoch 7/50\n",
      "5/5 - 0s - loss: 0.6263 - accuracy: 0.6721 - val_loss: 0.6061 - val_accuracy: 0.7049\n",
      "Epoch 8/50\n",
      "5/5 - 0s - loss: 0.6192 - accuracy: 0.6721 - val_loss: 0.5987 - val_accuracy: 0.7049\n",
      "Epoch 9/50\n",
      "5/5 - 0s - loss: 0.6120 - accuracy: 0.6782 - val_loss: 0.5898 - val_accuracy: 0.7049\n",
      "Epoch 10/50\n",
      "5/5 - 0s - loss: 0.6041 - accuracy: 0.6802 - val_loss: 0.5875 - val_accuracy: 0.7049\n",
      "Epoch 11/50\n",
      "5/5 - 0s - loss: 0.5975 - accuracy: 0.6843 - val_loss: 0.5786 - val_accuracy: 0.7049\n",
      "Epoch 12/50\n",
      "5/5 - 0s - loss: 0.5880 - accuracy: 0.6823 - val_loss: 0.5693 - val_accuracy: 0.7049\n",
      "Epoch 13/50\n",
      "5/5 - 0s - loss: 0.5790 - accuracy: 0.6884 - val_loss: 0.5644 - val_accuracy: 0.7049\n",
      "Epoch 14/50\n",
      "5/5 - 0s - loss: 0.5712 - accuracy: 0.6904 - val_loss: 0.5410 - val_accuracy: 0.7049\n",
      "Epoch 15/50\n",
      "5/5 - 0s - loss: 0.5627 - accuracy: 0.7088 - val_loss: 0.5429 - val_accuracy: 0.7049\n",
      "Epoch 16/50\n",
      "5/5 - 0s - loss: 0.5447 - accuracy: 0.7169 - val_loss: 0.5081 - val_accuracy: 0.7049\n",
      "Epoch 17/50\n",
      "5/5 - 0s - loss: 0.5320 - accuracy: 0.7373 - val_loss: 0.4953 - val_accuracy: 0.7869\n",
      "Epoch 18/50\n",
      "5/5 - 0s - loss: 0.5170 - accuracy: 0.7576 - val_loss: 0.4733 - val_accuracy: 0.7377\n",
      "Epoch 19/50\n",
      "5/5 - 0s - loss: 0.5033 - accuracy: 0.7637 - val_loss: 0.4633 - val_accuracy: 0.7705\n",
      "Epoch 20/50\n",
      "5/5 - 0s - loss: 0.4899 - accuracy: 0.7841 - val_loss: 0.4473 - val_accuracy: 0.7541\n",
      "Epoch 21/50\n",
      "5/5 - 0s - loss: 0.4821 - accuracy: 0.7943 - val_loss: 0.4518 - val_accuracy: 0.7869\n",
      "Epoch 22/50\n",
      "5/5 - 0s - loss: 0.4937 - accuracy: 0.7923 - val_loss: 0.4803 - val_accuracy: 0.7869\n",
      "Epoch 23/50\n",
      "5/5 - 0s - loss: 0.4747 - accuracy: 0.7841 - val_loss: 0.4462 - val_accuracy: 0.7705\n",
      "Epoch 24/50\n",
      "5/5 - 0s - loss: 0.4589 - accuracy: 0.7963 - val_loss: 0.4386 - val_accuracy: 0.7541\n",
      "Epoch 25/50\n",
      "5/5 - 0s - loss: 0.4575 - accuracy: 0.7963 - val_loss: 0.4388 - val_accuracy: 0.7377\n",
      "Epoch 26/50\n",
      "5/5 - 0s - loss: 0.4663 - accuracy: 0.7963 - val_loss: 0.4898 - val_accuracy: 0.7213\n",
      "Epoch 27/50\n",
      "5/5 - 0s - loss: 0.4547 - accuracy: 0.7902 - val_loss: 0.4397 - val_accuracy: 0.7541\n",
      "Epoch 28/50\n",
      "5/5 - 0s - loss: 0.4570 - accuracy: 0.7821 - val_loss: 0.4419 - val_accuracy: 0.7541\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2d64b6a7e50>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 100\n",
    "num_epochs = 50\n",
    "\n",
    "#set up an early stopping mechanism to avoid overfitting\n",
    "#the training process will stop the first time the validation loss increases\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(patience=4)\n",
    "\n",
    "model.fit(train_inputs, \n",
    "          train_targets, \n",
    "          batch_size = batch_size,\n",
    "          epochs = num_epochs,\n",
    "          callbacks = [early_stopping],\n",
    "          validation_data = (validation_inputs, validation_targets), \n",
    "          verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5008 - accuracy: 0.8065\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = model.evaluate(test_inputs, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss:0.50\n"
     ]
    }
   ],
   "source": [
    "print('Test loss:{0:.2f}'.format(test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:80.65%\n"
     ]
    }
   ],
   "source": [
    "print('Test Accuracy:{0:.2f}%'.format(test_accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
